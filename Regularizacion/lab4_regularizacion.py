# -*- coding: utf-8 -*-
"""Lab4 Regularizacion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ixMZ9sU6F2Armjzfw6YdG8MLSNSBB2Tm
"""

from google.colab import drive
drive.mount("/content/gdrive")

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
from matplotlib import pyplot
from scipy import optimize
# %matplotlib inline

"""# Regularización de regresión multivariable

El 80% de los datos de X serán utilizados para el entrenamiento que son 13900. El resto para la prueba
"""

data = np.loadtxt('/content/gdrive/MyDrive/Machine learning/Datasets/bicicletas_hora.txt', delimiter=',')
X_multivariable = data[:13900, :6]
y_multivariable = data[:13900, 6]

X_test_multivariable = data[13900:, :6]
Y_test_multivariable = data[13900:, 6]

m = y_multivariable.size

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])

    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

X_norm, mu, sigma = featureNormalize(X_multivariable.copy())

print(X_multivariable.copy())
print('Media calculada:', mu)
print('Desviación estandar calculada:', sigma)
print(X_norm)

X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)

print(X)

def computeCostMulti(X, y, theta):
    m = y.shape[0]
    J = 0
    h = np.dot(X, theta)
    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))
    return J

def gradientDescentMulti(X, y, theta, alpha, num_iters):
    m = y.shape[0]
    theta = theta.copy()
    J_history = []

    for i in range(num_iters):
        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)
        J_history.append(computeCostMulti(X, y, theta))

    return theta, J_history

alpha = 0.003
num_iters = 10000
theta_m = np.zeros(7)
theta_m, J_history = gradientDescentMulti(X, y_multivariable , theta_m, alpha, num_iters)

pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta_m)))

X_test_norm = X_test_multivariable.copy()
m, n = X_test_norm.shape
print(X_test_norm)

X_test_norm = np.concatenate([np.ones((m, 1)), X_test_norm], axis=1)
print(X_test_norm)

X_test_norm[:, 1:7] = (X_test_norm[:, 1:7] - mu) / sigma

print(X_test_norm)

pred = np.dot(X_test_norm, theta_m)
pred_int = pred.astype(int)

print(pred_int)
print(Y_test_multivariable)

print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred_int == Y_test_multivariable) * 100))

def computeCostMultiReg(X, y, theta, lambda_):
    m = y.shape[0]
    J = 0
    h = np.dot(X, theta)
    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y)) + (lambda_/(2 * m)) * np.sum(np.square(theta[1:]))
    return J

def gradientDescentMultiReg(X, y, theta, alpha, num_iters, lambda_):

    m = y.shape[0]

    theta = theta.copy()

    J_history = []

    for i in range(num_iters):
        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)
        theta[1:] = theta[1:] - (alpha / m) * lambda_ * theta[1:]
        J_history.append(computeCostMultiReg(X, y, theta, lambda_))

    return theta, J_history

alpha = 0.003
num_iters = 10000
theta_mr = np.zeros(7)
lambda_ = 0.01
theta_mr, J_history = gradientDescentMultiReg(X, y_multivariable , theta_mr, alpha, num_iters, lambda_)

pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta_mr)))

pred = np.dot(X_test_norm, theta_mr)
pred_int = pred.astype(int)

print(pred_int)
print(Y_test_multivariable)

print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred_int == Y_test_multivariable) * 100))

"""# Regularización de regresión logística

En la siguiente regresión se procederá a entrenarlo con el 80% de los datos ya tratados y realizar la prueba con el restante, para dicho cálculo el siguiente dataset cuenta con 61069 datos, por lo que el 80% 48855 aprox serán destinados al entrenamiento, mientras que el resto servirá para probar el entrenamiento.
"""

data = np.loadtxt('/content/gdrive/MyDrive/Machine learning/Datasets/secondary_mushroom_dataset.csv', delimiter=',')
y = data[:, 1]  # Primera columna
X = data[:, 2:]  # Resto de las columnas
# Divide los datos en conjuntos de entrenamiento y prueba
X_train, y_train = X[:48856], y[:48856]
X_test, y_test = X[48855:], y[48855:]
print(X_train)
print(y_train)

def sigmoid(z):
    z = np.array(z)
    g = np.zeros(z.shape)
    g = 1 / (1 + np.exp(-z))
    return g

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])

    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    print(sigma)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

X_norm, muL, sigmaL = featureNormalize(X_train.copy())

m, n = X_norm.shape
X_norm = np.concatenate([np.ones((m, 1)), X_norm], axis=1)

print(X_norm.shape)

def calcularCosto(theta, X, y):
    m = y.size
    J = 0
    h = sigmoid(X.dot(theta.T))
    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))

    return J

def descensoGradiente(theta, X, y, alpha, num_iters):
    m = y.shape[0]
    theta = theta.copy()
    J_history = []

    for i in range(num_iters):
        h = sigmoid(X.dot(theta.T))
        theta = theta - (alpha / m) * (h - y).dot(X)

        J_history.append(calcularCosto(theta, X, y))
    return theta, J_history

alpha = 0.03
num_iters = 6000
theta_l = np.zeros(19)
theta_l, J_history = descensoGradiente(theta_l, X_norm , y_train,  alpha, num_iters)
pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta_l)))

X_test_log = X_test.copy()
m, n = X_test_log.shape
print(X_test_log)

X_test_log = np.concatenate([np.ones((m, 1)), X_test_log], axis=1)
print(X_test_log.shape)
print(X_test_log)

X_test_log[:, 1:19] = (X_test_log[:, 1:19] - muL) / sigmaL
print(X_test_log)

"""La siguiente función tiene como objetivo colocar la probabilidad obteneida por la sigmoide aplicada en la hipótesis en 1 si es mayor o igual 0.4 y 0 si es menor, se decidió estos rangos por conveniencia."""

def predict(theta, X):
    probabilities = sigmoid(np.dot(X, theta))
    return [1 if x >= 0.4 else 0 for x in probabilities]

pred = predict(theta_l, X_test_log)

print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y_test) * 100))

def calcularCostoRegularizado(theta, X, y,lambda_):
    m = y.size
    J = 0
    grad = np.zeros(theta.shape)
    h = sigmoid(X.dot(theta.T))
    temp = theta.copy()
    temp[0] = 0
    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))
    grad = (1 / m) * (h - y).dot(X)
    grad = grad + (lambda_ / m) * temp

    return J, grad

def descensoGradienteRegularizado(X, y, theta, alpha, num_iters, lambda_):
    m = y.shape[0]
    J_history = np.zeros(num_iters)

    for i in range(num_iters):
        grad = calcularCostoRegularizado(theta, X, y, lambda_)[1]
        theta = theta - alpha * grad
        J_history[i] = calcularCostoRegularizado(theta, X, y, lambda_)[0]

    return theta, J_history

alpha = 0.03
num_iters = 6000
lambda_ = 10000
theta_lr = np.zeros(19)
theta_lr, J_history = descensoGradienteRegularizado( X_norm, y_train, theta_lr, alpha, num_iters, lambda_)
pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta_lr)))

pred = predict(theta_lr, X_test_log)

print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y_test) * 100))