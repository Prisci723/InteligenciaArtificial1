# -*- coding: utf-8 -*-
"""Regresion logistica mushroom.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zs9e4alVjVZLKL7fd596ERbKDfnZpBJC

# Dataset

El siguiente dataset cuenta con 18 características relacionadas a la estructura de un hongo, el fin de los datos es demostrar si un hongo es comestible o es venenoso.
El enlace al dataset original es: https://archive.ics.uci.edu/dataset/848/secondary+mushroom+dataset

Los features utilizados del dataset original son:


* x1: Diametro del sombrero del hongo, en cm
* x2: Forma del sombrero
* x3: Tipo de superficie
* x4: Color del sombrero
* x5: Si expulsa líquido al ser golpeado
* x6: Tipo de himenio
* x7: espacio entre himenio
* x8: color de himenio
* x9: Altura de estipe en cm
* x10: Ancho de estipe en mm
* x11: Tipo de raiz de estipe
* x12: Tipo de superficie de estipe
* x13: Color de la estipe
* x14: Si tiene anilo
* x15: Tipo de anillo
* x16: Color de las esporas
* x17: hábitat natural
* x18: Estación en la que aparece

# Tratamiento de los datos con pandas

Pandas es una librería que nos permite tratar con grandes cantidades de datos de forma más fácil y automatizada, en las siguientes partes limpiaré el dataset y haré algunos ajustes con la librería pandas.

El siguiente dataset cuenta con la mayoría de tipos de datos en categorías representados por un caracter. La librería pandas permitirá convertirlos en numéros para poder tratarlos.
"""

from google.colab import drive
drive.mount("/content/gdrive")

import pandas as pd

"""Primero se importa el archivo y se lo imprime, siendo leído por pandas"""

dataframe = pd.read_csv("/content/gdrive/MyDrive/Machine learning/Datasets/Secondary_Mushroom.txt", sep=";")
print(dataframe)

"""El método unique(), nos ayuda a distinguir los tipos de datos que existen en una determinada columna, por el tipo de nombre."""

dataframe['class'].unique()

"""Simplemente utilizando la función de replace en pandas, es que convertimos la y que es el resultado en valores 0 y 1. También se elimina dos columnas debido a que posee demasiado datos nulos que complicarán el resultado en el dataset, veil-color y veil-type."""

#Conversión de la y a binario
dataframe['class'].replace(['p', 'e'],
                        [0, 1], inplace=True)
print(dataframe['class'][1054:1070]) #Nos permite visualizar como datos cambiaron
dataframe.drop(['veil-type', 'veil-color'], axis=1)

"""Para poder tratar los datos y categorizarlos todos de forma automática también se puede utlizar otra librería denominada sklearn.preprocessing la cual nos permitirá colocar las cateogorias de caracterres dentro de las columnas como rangos numéricos"""

from sklearn.preprocessing import OrdinalEncoder

enc = OrdinalEncoder()

"""Dicho tratamiento se realizará utilizando el orden definido por OrdinalEncoder aplicado en todas las columnas necesarias"""

dataframe['cap-shape'] = enc.fit_transform(dataframe[['cap-shape']])
dataframe['cap-surface'] = enc.fit_transform(dataframe[['cap-surface']])
dataframe['cap-color'] = enc.fit_transform(dataframe[['cap-color']])
dataframe['does-bruise-or-bleed'] = enc.fit_transform(dataframe[['does-bruise-or-bleed']])
dataframe['gill-attachment'] = enc.fit_transform(dataframe[['gill-attachment']])
dataframe['gill-spacing'] = enc.fit_transform(dataframe[['gill-spacing']])
dataframe['gill-color'] = enc.fit_transform(dataframe[['gill-color']])
dataframe['stem-root'] = enc.fit_transform(dataframe[['stem-root']])
dataframe['stem-surface'] = enc.fit_transform(dataframe[['stem-surface']])
dataframe['stem-color'] = enc.fit_transform(dataframe[['stem-color']])
dataframe['has-ring'] = enc.fit_transform(dataframe[['has-ring']])
dataframe['ring-type'] = enc.fit_transform(dataframe[['ring-type']])
dataframe['spore-print-color'] = enc.fit_transform(dataframe[['spore-print-color']])
dataframe['habitat'] = enc.fit_transform(dataframe[['habitat']])
dataframe['season'] = enc.fit_transform(dataframe[['season']])

# Visualizar los datos ya cambiados
print(dataframe)

"""Hasta este punto es que se ha podido tratar los datos cambiándolos de caracteres a numéricos, pero dentro de este dataset también existen valores nulos, los cuales en el siguiente ejemplo serán reemplazados por la media de dicha columna para poder ayudar con el cálculo y pueda ser procesado."""

mean_gs = dataframe["gill-spacing"].mean()
dataframe["gill-spacing"] = dataframe["gill-spacing"].fillna(mean_gs)
mean_sr = dataframe["stem-root"].mean()
dataframe["stem-root"] = dataframe["stem-root"].fillna(mean_sr)
mean_ss = dataframe["stem-surface"].mean()
dataframe["stem-surface"] = dataframe["stem-surface"].fillna(mean_ss)
mean_sc = dataframe["spore-print-color"].mean()
dataframe["spore-print-color"] = dataframe["spore-print-color"].fillna(mean_sc)
mean_cs = dataframe["cap-surface"].mean()
dataframe["cap-surface"] = dataframe["cap-surface"].fillna(mean_cs)
mean_ga = dataframe["gill-attachment"].mean()
dataframe["gill-attachment"] = dataframe["gill-attachment"].fillna(mean_ga)
mean_rt = dataframe["ring-type"].mean()
dataframe["ring-type"] = dataframe["ring-type"].fillna(mean_rt)

"""Lo que el código hace es obtener la media de la columna y reemplazar los valores nulos con la media."""

print(dataframe)

"""Ya con los datos listos podemos guardarlos en un archivo y exportarlo para que sea utilizado y calcular los datos correspondientes"""

dataframe.to_csv("secondary_mushroom_dataset.csv")

"""# Regresion logística
En la siguiente regresión se procederá a entrenarlo con el 80% de los datos ya tratados y realizar la prueba con el restante, para dicho cálculo el siguiente dataset cuenta con 61069 datos, por lo que el 80% 48855 aprox serán destinados al entrenamiento, mientras que el resto servirá para probar el entrenamiento.
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
from matplotlib import pyplot
from scipy import optimize
# %matplotlib inline

"""La y se encuentra en la primera columna, pero debido al tratamiento de datos con pandas, es que se genera una columna extra de enumeramiento, es por eso que los datos son cargados apartir de la segunda posición 1, y los features comenzarían apartir de la tercera posición 2"""

data = np.loadtxt('/content/gdrive/MyDrive/Machine learning/Datasets/secondary_mushroom_dataset.csv', delimiter=',')
y = data[:, 1]  # Primera columna
X = data[:, 2:]  # Resto de las columnas
# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, y_train = X[:48856], y[:48856]
X_test, y_test = X[48855:], y[48855:]
X = X_train
y = y_train
print(X)
print(y)

"""Función de la sigmoide, permite calcular la probabilidad de que sea un determinado valor."""

def sigmoid(z):
    z = np.array(z)
    g = np.zeros(z.shape)
    g = 1 / (1 + np.exp(-z))
    return g

"""Los valores deben ser normalizados para no tener problemas con las distintas escalas utilizadas, por lo que se realiza el siguiente función y se la llama."""

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])

    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    print(sigma)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

X_norm, mu, sigma = featureNormalize(X)

"""Se añade la columna de x0 según en tamaño de la matriz"""

m, n = X.shape
X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)

"""Se definen las funciones de el descenso de gradiente y función de costo, esta vez utilizando las funciones necesarias para el tipo de resultado que se tendrá en y, que sería 0 o 1."""

def calcularCosto(theta, X, y):
    m = y.size
    J = 0
    h = sigmoid(X.dot(theta.T))
    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))

    return J

def descensoGradiente(theta, X, y, alpha, num_iters):
    m = y.shape[0]
    theta = theta.copy()
    J_history = []

    for i in range(num_iters):
        h = sigmoid(X.dot(theta.T))
        theta = theta - (alpha / m) * (h - y).dot(X)

        J_history.append(calcularCosto(theta, X, y))
    return theta, J_history

"""# Descenso de gradiente aplicado

Con las 4000 iteraciones y con un valor de alfa de 0.03 es que la función ya converge.
"""

alpha = 0.03
num_iters = 4000

theta = np.zeros(19)
theta, J_history = descensoGradiente(theta, X, y, alpha, num_iters)

pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta)))

"""Una vez obtenidos los valores de theta es que puede comenzar a realizarse la prueba, la cual será hecha con el 20% restante ya definidos al principio de la aplicación.
# Prueba de los datos

Primero los datos son normalizados
"""

X_test_norm, mu, sigma = featureNormalize(X_test)
m, n = X_test_norm.shape

"""Se añade la columna de x0 a x test"""

X_test_norm = np.concatenate([np.ones((m, 1)), X_test_norm], axis=1)

"""Se aplica la función de la sigmoide a los datos y luego se los visualiza"""

esComestible = sigmoid(np.dot(X_test_norm, theta))
print(esComestible)
print(y_test)

"""Podemos comprobar los valores de la prueba una vez que los datos de x_test están completos y también visualizar los datos de y_test

1 ------------> COMESTIBLE
0 ------------> VENENOSO

Podemos visualizar los datos obtnidos comparados con sus respectivas y para evaluar el entrenamiento
"""

X_test_resultado = esComestible.copy()
print(X_test_resultado.shape[0])

y_test_resultado = y_test.copy()
X_test_resultado= X_test_resultado.reshape(-1, 1)
y_test_resultado= y_test_resultado.reshape(-1, 1)
X_test_resultado = np.concatenate([ X_test_resultado, y_test_resultado], axis=1)

comparacion = pd.DataFrame(X_test_resultado)

print(comparacion)